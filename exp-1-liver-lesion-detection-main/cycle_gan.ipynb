{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"12vsNLEcWKOZqyMNjlS2khhjFkNx7m7wZ","authorship_tag":"ABX9TyPtZLX/ESNTSPWp1Eoq0GgU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","\n","file_path = '/content/drive/MyDrive/SuperAi/medical-ai/gan_dataset.zip'\n","if os.path.exists(file_path):\n","    print(\"File exists\")\n","else:\n","    print(\"File does not exist\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oDgFYuewBTHq","executionInfo":{"status":"ok","timestamp":1716974179492,"user_tz":-420,"elapsed":572,"user":{"displayName":"Graph Thongwat","userId":"10478732592282795937"}},"outputId":"f1773e1a-908d-4153-89c5-cdfb9c02fcbe"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["File exists\n"]}]},{"cell_type":"code","source":["!unzip -uq /content/drive/MyDrive/SuperAi/medical-ai/gan_dataset.zip"],"metadata":{"id":"dzJ0EaJ4dNgr","executionInfo":{"status":"ok","timestamp":1716974197447,"user_tz":-420,"elapsed":13323,"user":{"displayName":"Graph Thongwat","userId":"10478732592282795937"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"ok2FpE2bH0Ob","executionInfo":{"status":"ok","timestamp":1716948985894,"user_tz":-420,"elapsed":4133,"user":{"displayName":"Graph Thongwat","userId":"10478732592282795937"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import init\n","import functools\n","from torch.optim import lr_scheduler"]},{"cell_type":"code","source":["class ConvolutionalBlock(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels: int,\n","        out_channels: int,\n","        is_downsampling: bool = True,\n","        add_activation: bool = True,\n","        **kwargs\n","    ):\n","        super().__init__()\n","        if is_downsampling:\n","            self.conv = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs),\n","                nn.InstanceNorm2d(out_channels),\n","                nn.ReLU(inplace=True) if add_activation else nn.Identity(),\n","            )\n","        else:\n","            self.conv = nn.Sequential(\n","                nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n","                nn.InstanceNorm2d(out_channels),\n","                nn.ReLU(inplace=True) if add_activation else nn.Identity(),\n","            )\n","\n","    def forward(self, x):\n","        return self.conv(x)"],"metadata":{"id":"_7D2jIDyIlbi","executionInfo":{"status":"ok","timestamp":1716950021182,"user_tz":-420,"elapsed":7,"user":{"displayName":"Graph Thongwat","userId":"10478732592282795937"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class ResidualBlock(nn.Module):\n","    def __init__(self, channels: int):\n","        super().__init__()\n","        self.block = nn.Sequential(\n","            ConvolutionalBlock(channels, channels, add_activation=True, kernel_size=3, padding=1),\n","            ConvolutionalBlock(channels, channels, add_activation=False, kernel_size=3, padding=1),\n","        )\n","\n","    def forward(self, x):\n","        return x + self.block(x)"],"metadata":{"id":"Az5H6DrlMqni","executionInfo":{"status":"ok","timestamp":1716950055270,"user_tz":-420,"elapsed":6,"user":{"displayName":"Graph Thongwat","userId":"10478732592282795937"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class ConvInstanceNormLeakyReLUBlock(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels: int,\n","        out_channels: int,\n","        # is_downsampling: bool = True,\n","        add_activation: bool = True,\n","        **kwargs\n","    ):\n","        super().__init__()\n","\n","        self.conv = nn.Sequential(\n","            nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n","            nn.InstanceNorm2d(out_channels),\n","            nn.LeakyReLU(inplace=True) if add_activation else nn.Identity(),\n","        )\n","\n","    def forward(self, x):\n","        return self.conv(x)"],"metadata":{"id":"vgmmOczfUtEZ","executionInfo":{"status":"ok","timestamp":1716952393324,"user_tz":-420,"elapsed":8,"user":{"displayName":"Graph Thongwat","userId":"10478732592282795937"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class Generator(nn.Module):\n","    def __init__(\n","        self, img_channels: int, num_features: int = 64, num_residuals: int = 6\n","    ):\n","        \"\"\"\n","        Generator consists of 2 layers of downsampling/encoding layer,\n","        followed by 6 residual blocks for 128 × 128 training images\n","        and then 3 upsampling/decoding layer.\n","\n","        The network with 6 residual blocks can be written as:\n","        c7s1–64, d128, d256, R256, R256, R256, R256, R256, R256, u128, u64, and c7s1–3.\n","        \"\"\"\n","        super().__init__()\n","        self.initial_layer = nn.Sequential(\n","            nn.Conv2d(\n","                img_channels,\n","                num_features,\n","                kernel_size=7, # tune here try 5\n","                stride=1,\n","                padding=3,\n","                padding_mode=\"reflect\",\n","            ),\n","            nn.InstanceNorm2d(num_features),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","        self.downsampling_layers = nn.ModuleList(\n","            [\n","                ConvolutionalBlock(\n","                    num_features,\n","                    num_features * 2,\n","                    is_downsampling=True,\n","                    kernel_size=3,\n","                    stride=2,\n","                    padding=1,\n","                ),\n","                ConvolutionalBlock(\n","                    num_features * 2,\n","                    num_features * 4,\n","                    is_downsampling=True,\n","                    kernel_size=3,\n","                    stride=2,\n","                    padding=1,\n","                ),\n","            ]\n","        )\n","\n","        self.residual_layers = nn.Sequential(\n","            *[ResidualBlock(num_features * 4) for _ in range(num_residuals)]  # increase num_residual based on input image size\n","        )\n","\n","        self.upsampling_layers = nn.ModuleList(\n","            [\n","                ConvolutionalBlock(\n","                    num_features * 4,\n","                    num_features * 2,\n","                    is_downsampling=False,\n","                    kernel_size=3,\n","                    stride=2,\n","                    padding=1,\n","                    output_padding=1,\n","                ),\n","                ConvolutionalBlock(\n","                    num_features * 2,\n","                    num_features * 1,\n","                    is_downsampling=False,\n","                    kernel_size=3,\n","                    stride=2,\n","                    padding=1,\n","                    output_padding=1,\n","                ),\n","            ]\n","        )\n","\n","        self.last_layer = nn.Conv2d(\n","            num_features * 1,\n","            img_channels,\n","            kernel_size=7,\n","            stride=1,\n","            padding=3,\n","            padding_mode=\"reflect\",\n","        )\n","\n","    def forward(self, x):\n","        x = self.initial_layer(x)\n","        for layer in self.downsampling_layers:\n","            x = layer(x)\n","        x = self.residual_layers(x)\n","        for layer in self.upsampling_layers:\n","            x = layer(x)\n","        return torch.tanh(self.last_layer(x))"],"metadata":{"id":"9Lvcv-f6Mrjc","executionInfo":{"status":"ok","timestamp":1716950966413,"user_tz":-420,"elapsed":4,"user":{"displayName":"Graph Thongwat","userId":"10478732592282795937"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class Discriminator(nn.Module):\n","    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n","        super().__init__()\n","        self.initial_layer = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels,\n","                features[0],\n","                kernel_size=4,\n","                stride=2,\n","                padding=1,\n","                padding_mode=\"reflect\",\n","            ),\n","            nn.LeakyReLU(0.2, inplace=True),\n","        )\n","\n","        layers = []\n","        in_channels = features[0]\n","        for feature in features[1:]:\n","            layers.append(\n","                ConvInstanceNormLeakyReLUBlock(\n","                    in_channels,\n","                    feature,\n","                    stride=1 if feature == features[-1] else 2,\n","                )\n","            )\n","            in_channels = feature\n","\n","        layers.append(\n","            nn.Conv2d(\n","                in_channels,\n","                1,\n","                kernel_size=4,\n","                stride=1,\n","                padding=1,\n","                padding_mode=\"reflect\",\n","            )\n","        )\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.initial_layer(x)"],"metadata":{"id":"b5-0BNOIQQHL","executionInfo":{"status":"ok","timestamp":1716950994844,"user_tz":-420,"elapsed":5,"user":{"displayName":"Graph Thongwat","userId":"10478732592282795937"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import albumentations as A\n","from albumentations.pytorch import ToTensorV2"],"metadata":{"id":"TYQSbSsnYG0Z","executionInfo":{"status":"ok","timestamp":1716953055122,"user_tz":-420,"elapsed":2948,"user":{"displayName":"Graph Thongwat","userId":"10478732592282795937"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","import os\n","from torch.utils.data import Dataset\n","import numpy as np\n","from torchvision import transforms\n","\n","class HorseZebraDataset(Dataset):\n","    def __init__(self, root_zebra, root_horse, transform=None):\n","        self.root_zebra = root_zebra\n","        self.root_horse = root_horse\n","        self.transform = transform\n","\n","        self.zebra_images = os.listdir(root_zebra)\n","        self.horse_images = os.listdir(root_horse)\n","        self.length_dataset = max(len(self.zebra_images), len(self.horse_images))\n","        self.zebra_len = len(self.zebra_images)\n","        self.horse_len = len(self.horse_images)\n","\n","    def __len__(self):\n","        return self.length_dataset\n","\n","    def __getitem__(self, index):\n","        zebra_img = self.zebra_images[index % self.zebra_len]\n","        horse_img = self.horse_images[index % self.horse_len]\n","\n","        zebra_path = os.path.join(self.root_zebra, zebra_img)\n","        horse_path = os.path.join(self.root_horse, horse_img)\n","\n","        zebra_img = np.array(Image.open(zebra_path).convert(\"RGB\"))\n","        horse_img = np.array(Image.open(horse_path).convert(\"RGB\"))\n","\n","        if self.transform:\n","            augmentations = self.transform(image=zebra_img, image0=horse_img)\n","            zebra_img = augmentations[\"image\"]\n","            horse_img = augmentations[\"image0\"]\n","\n","        return zebra_img, horse_img"],"metadata":{"id":"CS5lCJbpWNV-","executionInfo":{"status":"ok","timestamp":1716952990157,"user_tz":-420,"elapsed":2606,"user":{"displayName":"Graph Thongwat","userId":"10478732592282795937"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["transforms = A.Compose(\n","    [\n","        A.Resize(width=256, height=256),\n","        A.HorizontalFlip(p=0.5),\n","        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255),\n","        ToTensorV2(),\n","    ],\n","    additional_targets={\"image0\": \"image\"},\n",")"],"metadata":{"id":"mKt-JUaSWVSC","executionInfo":{"status":"ok","timestamp":1716953062132,"user_tz":-420,"elapsed":4,"user":{"displayName":"Graph Thongwat","userId":"10478732592282795937"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"e1WMfNVeYS0g","executionInfo":{"status":"ok","timestamp":1716953172795,"user_tz":-420,"elapsed":488,"user":{"displayName":"Graph Thongwat","userId":"10478732592282795937"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import torch\n","import sys\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","from torchvision.utils import save_image\n","\n","def train_fn(\n","    disc_H, disc_Z, gen_Z, gen_H, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler\n","):\n","    H_reals = 0\n","    H_fakes = 0\n","    loop = tqdm(loader, leave=True)\n","\n","    for idx, (zebra, horse) in enumerate(loop):\n","        zebra = zebra.to(DEVICE)\n","        horse = horse.to(DEVICE)\n","\n","        # Train discriminators H and Z\n","        with torch.cuda.amp.autocast():\n","            fake_horse = gen_H(zebra)\n","            D_H_real = disc_H(horse)\n","            D_H_fake = disc_H(fake_horse.detach())\n","            H_reals += D_H_real.mean().item()\n","            H_fakes += D_H_fake.mean().item()\n","            D_H_real_loss = mse(D_H_real, torch.ones_like(D_H_real))\n","            D_H_fake_loss = mse(D_H_fake, torch.zeros_like(D_H_fake))\n","            D_H_loss = D_H_real_loss + D_H_fake_loss\n","\n","            fake_zebra = gen_Z(horse)\n","            D_Z_real = disc_Z(zebra)\n","            D_Z_fake = disc_Z(fake_zebra.detach())\n","            D_Z_real_loss = mse(D_Z_real, torch.ones_like(D_Z_real))\n","            D_Z_fake_loss = mse(D_Z_fake, torch.zeros_like(D_Z_fake))\n","            D_Z_loss = D_Z_real_loss + D_Z_fake_loss\n","\n","            D_loss = (D_H_loss + D_Z_loss) / 2\n","\n","        opt_disc.zero_grad()\n","        d_scaler.scale(D_loss).backward()\n","        d_scaler.step(opt_disc)\n","        d_scaler.update()\n","\n","        # Train generators H and Z\n","        with torch.cuda.amp.autocast():\n","            # adversarial losses\n","            D_H_fake = disc_H(fake_horse)\n","            D_Z_fake = disc_Z(fake_zebra)\n","            loss_G_H = mse(D_H_fake, torch.ones_like(D_H_fake))\n","            loss_G_Z = mse(D_Z_fake, torch.ones_like(D_Z_fake))\n","\n","            # cycle losses\n","            cycle_zebra = gen_Z(fake_horse)\n","            cycle_horse = gen_H(fake_zebra)\n","            cycle_zebra_loss = l1(zebra, cycle_zebra)\n","            cycle_horse_loss = l1(horse, cycle_horse)\n","\n","            # total loss\n","            G_loss = (\n","                loss_G_Z\n","                + loss_G_H\n","                + cycle_zebra_loss * LAMBDA_CYCLE\n","                + cycle_horse_loss * LAMBDA_CYCLE\n","            )\n","\n","        opt_gen.zero_grad()\n","        g_scaler.scale(G_loss).backward()\n","        g_scaler.step(opt_gen)\n","        g_scaler.update()\n","\n","        if idx % 200 == 0:\n","            save_image(fake_horse * 0.5 + 0.5, f\"outputs/horse_{idx}.png\")\n","            save_image(fake_zebra * 0.5 + 0.5, f\"outputs/zebra_{idx}.png\")\n","\n","        loop.set_postfix(H_real=H_reals / (idx + 1), H_fake=H_fakes / (idx + 1))"],"metadata":{"id":"Mbi3uTkSYLUn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TRAIN_DIR = \"data/train\"\n","VAL_DIR = \"data/val\"\n","BATCH_SIZE = 1\n","LEARNING_RATE = 1e-5\n","LAMBDA_CYCLE = 10\n","NUM_WORKERS = 4\n","NUM_EPOCHS = 50"],"metadata":{"id":"eSji5dXdgucX","executionInfo":{"status":"ok","timestamp":1716972095335,"user_tz":-420,"elapsed":606,"user":{"displayName":"Graph Thongwat","userId":"10478732592282795937"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def main():\n","    disc_H = Discriminator(in_channels=3).to(DEVICE)\n","    disc_Z = Discriminator(in_channels=3).to(DEVICE)\n","    gen_Z = Generator(img_channels=3, num_residuals=9).to(DEVICE)\n","    gen_H = Generator(img_channels=3, num_residuals=9).to(DEVICE)\n","\n","    # use Adam Optimizer for both generator and discriminator\n","    opt_disc = optim.Adam(\n","        list(disc_H.parameters()) + list(disc_Z.parameters()),\n","        lr=LEARNING_RATE,\n","        betas=(0.5, 0.999),\n","    )\n","\n","    opt_gen = optim.Adam(\n","        list(gen_Z.parameters()) + list(gen_H.parameters()),\n","        lr=LEARNING_RATE,\n","        betas=(0.5, 0.999),\n","    )\n","\n","    L1 = nn.L1Loss()\n","    mse = nn.MSELoss()\n","\n","    # if LOAD_MODEL:\n","    #     load_checkpoint(\n","    #         CHECKPOINT_GENERATOR_H,\n","    #         gen_H,\n","    #         opt_gen,\n","    #         LEARNING_RATE,\n","    #     )\n","    #     load_checkpoint(\n","    #         CHECKPOINT_GENERATOR_Z,\n","    #         gen_Z,\n","    #         opt_gen,\n","    #         LEARNING_RATE,\n","    #     )\n","    #     load_checkpoint(\n","    #         CHECKPOINT_DISCRIMINATOR_H,\n","    #         disc_H,\n","    #         opt_disc,\n","    #         LEARNING_RATE,\n","    #     )\n","    #     load_checkpoint(\n","    #         CHECKPOINT_DISCRIMINATOR_Z,\n","    #         disc_Z,\n","    #         opt_disc,\n","    #         LEARNING_RATE,\n","    #     )\n","\n","    dataset = HorseZebraDataset(\n","        root_horse=TRAIN_DIR + \"/horses\",\n","        root_zebra=TRAIN_DIR + \"/zebras\",\n","        transform=transforms,\n","    )\n","    val_dataset = HorseZebraDataset(\n","        root_horse=VAL_DIR + \"/horses\",\n","        root_zebra=VAL_DIR + \"/zebras\",\n","        transform=transforms,\n","    )\n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=BATCH_SIZE,\n","        shuffle=False,\n","        pin_memory=True,\n","    )\n","    loader = DataLoader(\n","        dataset,\n","        batch_size=BATCH_SIZE,\n","        shuffle=True,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=True,\n","    )\n","    g_scaler = torch.cuda.amp.GradScaler()\n","    d_scaler = torch.cuda.amp.GradScaler()\n","\n","    for epoch in range(NUM_EPOCHS):\n","        train_fn(\n","            disc_H,\n","            disc_Z,\n","            gen_Z,\n","            gen_H,\n","            loader,\n","            opt_disc,\n","            opt_gen,\n","            L1,\n","            mse,\n","            d_scaler,\n","            g_scaler,\n","        )\n","\n","        if SAVE_MODEL:\n","            save_checkpoint(gen_H, opt_gen, filename=CHECKPOINT_GENERATOR_H)\n","            save_checkpoint(gen_Z, opt_gen, filename=CHECKPOINT_GENERATOR_Z)\n","            save_checkpoint(disc_H, opt_disc, filename=CHECKPOINT_DISCRIMINATOR_H)\n","            save_checkpoint(disc_Z, opt_disc, filename=CHECKPOINT_DISCRIMINATOR_Z)\n","\n","\n","# # run the training phase\n","# main()"],"metadata":{"id":"ZFq2rwZkgkRL","executionInfo":{"status":"ok","timestamp":1716972073501,"user_tz":-420,"elapsed":1311,"user":{"displayName":"Graph Thongwat","userId":"10478732592282795937"}}},"execution_count":5,"outputs":[]}]}